{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbJSgH5DKbqq",
        "outputId": "15adc8aa-e981-431b-b864-137bc9de8c59"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEAh-9m9MWl5"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import pkg_resources\n",
        "import pickle\n",
        "import nltk\n",
        "import re, string, json\n",
        "from tqdm.notebook import tqdm\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjlXYTtaKvAL"
      },
      "outputs": [],
      "source": [
        "from numpy import array\n",
        "from pickle import dump\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9JJQyhHKxv3"
      },
      "outputs": [],
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # replace '--' with a space ' '\n",
        "    doc = doc.replace('--', ' ')\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kju8JRUSLTHr"
      },
      "outputs": [],
      "source": [
        "def tokenize_twitter(sentences):\n",
        "    \"\"\"\n",
        "    Tokenize sentences into tokens (words)\n",
        "    \n",
        "    Args:\n",
        "        sentences: List of strings\n",
        "    \n",
        "    Returns:\n",
        "        List of lists of tokens\n",
        "    \"\"\"\n",
        "    print(\"Starting Cleaning Process\")\n",
        "    tokenized_sentences = []\n",
        "    for sentence in tqdm(sentences):\n",
        "        \n",
        "        # Convert to lowercase letters\n",
        "        sentence = cleanhtml(sentence)\n",
        "        sentence = _replace_urls(sentence)\n",
        "        sentence = remove_email(sentence)\n",
        "        sentence = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
        "        sentence = sentence.lower()\n",
        "        sentence = misc(sentence)\n",
        "        \n",
        "\n",
        "        # tokenized = nltk.word_tokenize(sentence)\n",
        "        \n",
        "        # append the list of words to the list of lists\n",
        "        # tokenized_sentences.append(tokenized)\n",
        "        tokenized_sentences.append(sentence)\n",
        "    \n",
        "    return tokenized_sentences\n",
        "\n",
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext\n",
        "\n",
        "\n",
        "def _replace_urls(data):\n",
        "    #Removing URLs with a regular expression\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    data = url_pattern.sub(r'', data)\n",
        "    return data\n",
        "\n",
        "def remove_email(data):\n",
        "    # Remove Emails\n",
        "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
        "    return data\n",
        "\n",
        "def misc(data):\n",
        "    # Remove new line characters\n",
        "    data = re.sub('\\s+', ' ', data)\n",
        "    # Remove distracting single quotes\n",
        "    data = re.sub(\"\\'\", \"\", data)\n",
        "    data = re.sub(\"ww+\", \"\", data)\n",
        "    # Removing roman-case:\n",
        "    MAYBE_ROMAN = re.compile(r'(\\b[MDCLXVI]+\\b)(\\.)?', re.I)\n",
        "    data = re.sub(MAYBE_ROMAN, \"\", data)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdzDdtS6M3NW"
      },
      "outputs": [],
      "source": [
        "def littleCleaning(sentences):\n",
        "    print(\"Starting cleaning Process\")\n",
        "    ret_list = []\n",
        "    for sentence in sentences:\n",
        "      words = sentence.split(\" \")\n",
        "      if len(words) > 5:\n",
        "        ret_list.append(sentence)\n",
        "      else:\n",
        "        continue\n",
        "    return ret_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg8vn9YuMI9f"
      },
      "outputs": [],
      "source": [
        "# nltk.download('wordnet')\n",
        "# nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dsejedsMLRR",
        "outputId": "5a0cf250-3dd9-4bc6-bf39-d51a950e0db3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of the corpus is: : 1174387\n"
          ]
        }
      ],
      "source": [
        "path = 'drive/MyDrive/Dataset/republic.txt'\n",
        "text = open(path).read().lower()\n",
        "print('length of the corpus is: :', len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCNWKWO_MhcK",
        "outputId": "e4fa1fe4-4829-485f-b42b-f53ca49b7d07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the project gutenberg ebook of the republic, by plato\\n\\nthis ebook is for the use of anyone anywhere in the united states and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever',\n",
              " ' you may copy it, give it away or re-use it under the terms\\nof the project gutenberg license included with this ebook or online at\\nwww',\n",
              " 'gutenberg',\n",
              " 'org',\n",
              " ' if you are not located in the united states, you\\nwill have to check the laws of the country where you are located before\\nusing this ebook',\n",
              " '\\n\\ntitle: the republic\\n\\nauthor: plato\\n\\ntranslator: b',\n",
              " ' jowett\\n\\nrelease date: october, 1998 [ebook #1497]\\n[most recently updated: september 11, 2021]\\n\\nlanguage: english\\n\\n\\nproduced by: sue asscher and david widger\\n\\n*** start of the project gutenberg ebook the republic ***\\n\\n\\n\\n\\nthe republic\\n\\nby plato\\n\\ntranslated by benjamin jowett\\n\\nnote: see also “the republic” by plato, jowett, ebook #150\\n\\n\\ncontents\\n\\n introduction and analysis',\n",
              " '\\n the republic',\n",
              " '\\n persons of the dialogue',\n",
              " '\\n book i',\n",
              " '\\n book ii',\n",
              " '\\n book iii',\n",
              " '\\n book iv',\n",
              " '\\n book v',\n",
              " '\\n book vi',\n",
              " '\\n book vii',\n",
              " '\\n book viii',\n",
              " '\\n book ix',\n",
              " '\\n book x',\n",
              " '\\n\\n\\n\\n\\n introduction and analysis']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Converting the data into lists.\n",
        "\n",
        "data_list = text.split(\".\")\n",
        "data_list[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "c14c6d187eb64f729cc53fa007733e27",
            "956f50b5dafa484a9eee9a9f52a4bac3",
            "a8e83132c4a24ec4b2d1653ece9fbd1d",
            "a2e72ed1e4ca4485b9ecc73f91e3d59e",
            "b1d571d9f1d24c96a3d96bfdd98a5e4f",
            "8a93bf256083460a8090989c6c4b0883",
            "29b4e1545e5940da9e15399f19ee5840",
            "abbf38b03dce49ad866e552fb229cd5b",
            "33fb7c12105c4d2094f65e7b47704d66",
            "ca5ada46b47b48dfbf8e1659014c6bea",
            "600bf0cea5374371b623d699c9f1f77e"
          ]
        },
        "id": "suY56Yf2Msxd",
        "outputId": "ba1d0fff-8fdb-439f-b30b-5908001958fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Normalization Process\n",
            "Starting Cleaning Process\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c14c6d187eb64f729cc53fa007733e27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7012 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting cleaning Process\n",
            "Normalization Process Finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['the project gutenberg ebook of the republic by plato this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever',\n",
              " ' you may copy it give it away or re use it under the terms of the project gutenberg license included with this ebook or online at ',\n",
              " ' if you are not located in the united states you will have to check the laws of the country where you are located before using this ebook',\n",
              " ' title the republic author plato translator b',\n",
              " ' jowett release date october ebook most recently updated september language english produced by sue asscher and david widger start of the project gutenberg ebook the republic the republic by plato translated by benjamin jowett note see also the republic by plato jowett ebook contents introduction and analysis']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pro_sentences = []\n",
        "\n",
        "def normalization_pipeline(sentences):\n",
        "    print(\"Starting Normalization Process\")\n",
        "    sentences = tokenize_twitter(sentences)\n",
        "    sentences = littleCleaning(sentences)\n",
        "    print(\"Normalization Process Finished\")\n",
        "    return sentences\n",
        "\n",
        "pro_sentences = normalization_pipeline(data_list)\n",
        "pro_sentences[: 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27mrc_XQQKh1",
        "outputId": "73ab968d-d199-495d-96be-91945d435683"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6309"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(pro_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "2xBGuKueMzA_",
        "outputId": "99b57b06-9652-47c0-f687-f83a749b1038"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the project gutenberg ebook of the republic by plato this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions what'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Structuring th etext into a paragraph:\n",
        "\n",
        "dataText = \"\".join(pro_sentences[: 700])\n",
        "dataText[: 200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8_pSH7pNSgY"
      },
      "outputs": [],
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # replace '--' with a space ' '\n",
        "    doc = doc.replace('--', ' ')\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi2-sYwTNW1L",
        "outputId": "bf99516b-fd11-44a6-9124-6149f0032658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'republic', 'by', 'plato', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'title', 'the', 'republic', 'author', 'plato', 'translator', 'b', 'jowett', 'release', 'date', 'october', 'ebook', 'most', 'recently', 'updated', 'september', 'language', 'english', 'produced', 'by', 'sue', 'asscher', 'and', 'david', 'widger', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'the', 'republic', 'the', 'republic', 'by', 'plato', 'translated', 'by', 'benjamin', 'jowett', 'note', 'see', 'also', 'the', 'republic', 'by', 'plato', 'jowett', 'ebook', 'contents', 'introduction', 'and', 'analysis', 'the', 'republic', 'of', 'plato', 'is', 'the', 'longest', 'of', 'his', 'works', 'with', 'the', 'exception', 'of', 'the', 'laws', 'and', 'is', 'certainly', 'the', 'greatest', 'of', 'them', 'there', 'are', 'nearer', 'approaches', 'to', 'modern', 'metaphysics', 'in', 'the', 'philebus', 'and', 'in', 'the', 'sophist', 'the', 'politicus', 'or', 'statesman', 'is', 'more', 'ideal', 'the', 'form', 'and', 'institutions', 'of', 'the', 'state', 'are', 'more', 'clearly', 'drawn']\n",
            "Total Tokens: 21735\n",
            "Unique Tokens: 3548\n"
          ]
        }
      ],
      "source": [
        "# clean document\n",
        "tokens = clean_doc(dataText)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuD679t_Na-n",
        "outputId": "75284862-24fb-4357-e9bc-5016b12fa177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Sequences: 21684\n"
          ]
        }
      ],
      "source": [
        "# organize into sequences of tokens\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "    # select sequence of tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # convert into a line\n",
        "    line = ' '.join(seq)\n",
        "    # store\n",
        "    sequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfJ8XWP7NjUS"
      },
      "outputs": [],
      "source": [
        "# save sequences to file\n",
        "\n",
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "    \n",
        "out_filename = 'drive/MyDrive/Dataset/republic_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1xwkNOvNsP_"
      },
      "outputs": [],
      "source": [
        "# load\n",
        "in_filename = 'drive/MyDrive/Dataset/republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuBjmCLNN2X3",
        "outputId": "f9e1e438-531f-431c-fc66-1970aa4e947e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 50, 50)            177450    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 50, 50)            20200     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 50)                20200     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3549)              180999    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 401,399\n",
            "Trainable params: 401,399\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "170/170 [==============================] - 25s 120ms/step - loss: 6.6283 - accuracy: 0.0831\n",
            "Epoch 2/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 6.1222 - accuracy: 0.0843\n",
            "Epoch 3/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 5.9787 - accuracy: 0.0904\n",
            "Epoch 4/50\n",
            "170/170 [==============================] - 20s 118ms/step - loss: 5.8110 - accuracy: 0.1136\n",
            "Epoch 5/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 5.6685 - accuracy: 0.1232\n",
            "Epoch 6/50\n",
            "170/170 [==============================] - 22s 129ms/step - loss: 5.5670 - accuracy: 0.1287\n",
            "Epoch 7/50\n",
            "170/170 [==============================] - 21s 123ms/step - loss: 5.4900 - accuracy: 0.1319\n",
            "Epoch 8/50\n",
            "170/170 [==============================] - 21s 121ms/step - loss: 5.4197 - accuracy: 0.1360\n",
            "Epoch 9/50\n",
            "170/170 [==============================] - 21s 123ms/step - loss: 5.3558 - accuracy: 0.1394\n",
            "Epoch 10/50\n",
            "170/170 [==============================] - 22s 128ms/step - loss: 5.3001 - accuracy: 0.1426\n",
            "Epoch 11/50\n",
            "170/170 [==============================] - 22s 128ms/step - loss: 5.2517 - accuracy: 0.1461\n",
            "Epoch 12/50\n",
            "170/170 [==============================] - 21s 125ms/step - loss: 5.2090 - accuracy: 0.1487\n",
            "Epoch 13/50\n",
            "170/170 [==============================] - 22s 128ms/step - loss: 5.1653 - accuracy: 0.1537\n",
            "Epoch 14/50\n",
            "170/170 [==============================] - 21s 126ms/step - loss: 5.1244 - accuracy: 0.1563\n",
            "Epoch 15/50\n",
            "170/170 [==============================] - 21s 122ms/step - loss: 5.0810 - accuracy: 0.1600\n",
            "Epoch 16/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 5.0371 - accuracy: 0.1630\n",
            "Epoch 17/50\n",
            "170/170 [==============================] - 20s 120ms/step - loss: 4.9889 - accuracy: 0.1669\n",
            "Epoch 18/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 4.9458 - accuracy: 0.1706\n",
            "Epoch 19/50\n",
            "170/170 [==============================] - 21s 122ms/step - loss: 4.8946 - accuracy: 0.1740\n",
            "Epoch 20/50\n",
            "170/170 [==============================] - 21s 121ms/step - loss: 4.8453 - accuracy: 0.1780\n",
            "Epoch 21/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 4.7958 - accuracy: 0.1814\n",
            "Epoch 22/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 4.7466 - accuracy: 0.1833\n",
            "Epoch 23/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 4.6949 - accuracy: 0.1863\n",
            "Epoch 24/50\n",
            "170/170 [==============================] - 20s 120ms/step - loss: 4.6473 - accuracy: 0.1906\n",
            "Epoch 25/50\n",
            "170/170 [==============================] - 21s 121ms/step - loss: 4.5978 - accuracy: 0.1916\n",
            "Epoch 26/50\n",
            "170/170 [==============================] - 21s 121ms/step - loss: 4.5498 - accuracy: 0.1951\n",
            "Epoch 27/50\n",
            "170/170 [==============================] - 20s 120ms/step - loss: 4.5037 - accuracy: 0.1981\n",
            "Epoch 28/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 4.4519 - accuracy: 0.2008\n",
            "Epoch 29/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 4.4069 - accuracy: 0.2031\n",
            "Epoch 30/50\n",
            "170/170 [==============================] - 20s 118ms/step - loss: 4.3608 - accuracy: 0.2070\n",
            "Epoch 31/50\n",
            "170/170 [==============================] - 20s 120ms/step - loss: 4.3129 - accuracy: 0.2091\n",
            "Epoch 32/50\n",
            "170/170 [==============================] - 20s 118ms/step - loss: 4.2663 - accuracy: 0.2122\n",
            "Epoch 33/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 4.2238 - accuracy: 0.2161\n",
            "Epoch 34/50\n",
            "170/170 [==============================] - 20s 118ms/step - loss: 4.1812 - accuracy: 0.2183\n",
            "Epoch 35/50\n",
            "170/170 [==============================] - 20s 120ms/step - loss: 4.1387 - accuracy: 0.2209\n",
            "Epoch 36/50\n",
            "170/170 [==============================] - 20s 118ms/step - loss: 4.0970 - accuracy: 0.2239\n",
            "Epoch 37/50\n",
            "170/170 [==============================] - 20s 120ms/step - loss: 4.0553 - accuracy: 0.2276\n",
            "Epoch 38/50\n",
            "170/170 [==============================] - 20s 120ms/step - loss: 4.0153 - accuracy: 0.2285\n",
            "Epoch 39/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 3.9730 - accuracy: 0.2336\n",
            "Epoch 40/50\n",
            "170/170 [==============================] - 20s 120ms/step - loss: 3.9365 - accuracy: 0.2352\n",
            "Epoch 41/50\n",
            "170/170 [==============================] - 20s 118ms/step - loss: 3.8928 - accuracy: 0.2390\n",
            "Epoch 42/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 3.8526 - accuracy: 0.2400\n",
            "Epoch 43/50\n",
            "170/170 [==============================] - 21s 122ms/step - loss: 3.8142 - accuracy: 0.2449\n",
            "Epoch 44/50\n",
            "170/170 [==============================] - 20s 121ms/step - loss: 3.7788 - accuracy: 0.2471\n",
            "Epoch 45/50\n",
            "170/170 [==============================] - 21s 125ms/step - loss: 3.7364 - accuracy: 0.2492\n",
            "Epoch 46/50\n",
            "170/170 [==============================] - 21s 121ms/step - loss: 3.7004 - accuracy: 0.2534\n",
            "Epoch 47/50\n",
            "170/170 [==============================] - 20s 120ms/step - loss: 3.6622 - accuracy: 0.2575\n",
            "Epoch 48/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 3.6262 - accuracy: 0.2591\n",
            "Epoch 49/50\n",
            "170/170 [==============================] - 20s 119ms/step - loss: 3.5933 - accuracy: 0.2647\n",
            "Epoch 50/50\n",
            "170/170 [==============================] - 20s 120ms/step - loss: 3.5530 - accuracy: 0.2711\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f342e611a50>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "batch_size=128\n",
        "epochs=50\n",
        "model.fit(X, y, batch_size=batch_size, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dTeDGBkN39c"
      },
      "outputs": [],
      "source": [
        "# save the model to file\n",
        "model.save(\"drive/MyDrive/Dataset/DataScience-Pianalytix-Models/nexWordPredict/nextWord.h5\")\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('drive/MyDrive/Dataset/DataScience-Pianalytix-Models/tokenizer.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyS3h_cHPSjB"
      },
      "outputs": [],
      "source": [
        "# generate a sequence from a language model\n",
        "import numpy as np\n",
        "\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "    result = list()\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "        # encode the text as integer\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict probabilities for each word\n",
        "        # yhat = model.predict_classes(encoded, verbose=0)\n",
        "        predict_x=model.predict(encoded) \n",
        "        yhat=np.argmax(predict_x,axis=1)\n",
        "        # map predicted word index to word\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCkTMAUeXhTb",
        "outputId": "859bab37-44fb-4164-d339-a23e883f87bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21684\n",
            "the project gutenberg ebook of the republic by plato this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it give it away or re use it under the\n"
          ]
        }
      ],
      "source": [
        "# load cleaned text sequences\n",
        "in_filename = 'drive/MyDrive/Dataset/republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "seq_length = len(lines[0].split()) - 1\n",
        "\n",
        "print(len(lines))\n",
        "print(lines[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljsZ7iI0Xra8",
        "outputId": "eaddaf21-827a-42f3-a149-1b5fb718ae76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "after ages which few great writers have ever been able to anticipate for themselves they do not perceive the want of connexion in their own writings or the gaps in their systems which are visible enough to those who come after them in the beginnings of literature and philosophy amid the\n",
            "\n",
            "best of good the republic is not not to be the embodiment\n"
          ]
        }
      ],
      "source": [
        "# load the model\n",
        "model = load_model(\"drive/MyDrive/Dataset/DataScience-Pianalytix-Models/nexWordPredict/nextWord.h5\")\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('drive/MyDrive/Dataset/DataScience-Pianalytix-Models/tokenizer.pkl', 'rb'))\n",
        "\n",
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 12)\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb1g4owwX5Bq"
      },
      "outputs": [],
      "source": [
        "# tt = \"this is not going to work because as i can see this to be going\"\n",
        "# seq_length = len(tt.split()) - 1\n",
        "\n",
        "# generate new text\n",
        "# generated = generate_seq(model, tokenizer, seq_length, tt, 12)\n",
        "# print(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAxYTMVriL1b"
      },
      "outputs": [],
      "source": [
        "# The above code will give error as it the string is only 15 tokens long, while we require 50 token long strings."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Predict-words.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "29b4e1545e5940da9e15399f19ee5840": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33fb7c12105c4d2094f65e7b47704d66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "600bf0cea5374371b623d699c9f1f77e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a93bf256083460a8090989c6c4b0883": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "956f50b5dafa484a9eee9a9f52a4bac3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2e72ed1e4ca4485b9ecc73f91e3d59e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33fb7c12105c4d2094f65e7b47704d66",
            "max": 7012,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abbf38b03dce49ad866e552fb229cd5b",
            "value": 7012
          }
        },
        "a8e83132c4a24ec4b2d1653ece9fbd1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b4e1545e5940da9e15399f19ee5840",
            "placeholder": "​",
            "style": "IPY_MODEL_8a93bf256083460a8090989c6c4b0883",
            "value": "100%"
          }
        },
        "abbf38b03dce49ad866e552fb229cd5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1d571d9f1d24c96a3d96bfdd98a5e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_600bf0cea5374371b623d699c9f1f77e",
            "placeholder": "​",
            "style": "IPY_MODEL_ca5ada46b47b48dfbf8e1659014c6bea",
            "value": " 7012/7012 [00:00&lt;00:00, 15333.53it/s]"
          }
        },
        "c14c6d187eb64f729cc53fa007733e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8e83132c4a24ec4b2d1653ece9fbd1d",
              "IPY_MODEL_a2e72ed1e4ca4485b9ecc73f91e3d59e",
              "IPY_MODEL_b1d571d9f1d24c96a3d96bfdd98a5e4f"
            ],
            "layout": "IPY_MODEL_956f50b5dafa484a9eee9a9f52a4bac3"
          }
        },
        "ca5ada46b47b48dfbf8e1659014c6bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
